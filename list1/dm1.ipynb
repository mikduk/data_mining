{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "0ce438c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stemming.porter2 import stem\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "\n",
    "PATH = 'data'\n",
    "BOOK_NAME = 'book'\n",
    "FILE_EXTENSION = \"txt\"\n",
    "\n",
    "NUMBER_OF_CHAPTERS = 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6037b1",
   "metadata": {},
   "source": [
    "#### Divide book by chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "29d24597",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+'/'+BOOK_NAME+'.'+FILE_EXTENSION, encoding=\"UTF-8\") as f:\n",
    "    i = 0\n",
    "    new_chapter = False\n",
    "    fx = open(PATH+'/'+BOOK_NAME+\"_00.\"+FILE_EXTENSION, 'w')\n",
    "    for line in f:\n",
    "        if \"CHAPTER\" in line:\n",
    "            i += 1\n",
    "            new_chapter = True\n",
    "        if new_chapter:\n",
    "           new_chapter = False \n",
    "           fx.close()\n",
    "           if i < 10:\n",
    "            z = \"0\" + str(i)\n",
    "           else:\n",
    "            z = str(i) \n",
    "           fx = open(PATH+'/'+BOOK_NAME+'_'+z+'.'+FILE_EXTENSION, 'w') \n",
    "        fx.write(line)\n",
    "    fx.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c1a1a",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "ff757b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+'/stopwords.'+FILE_EXTENSION, encoding=\"UTF-8\") as f:\n",
    "    stopwords = [word for line in f for word in line.split()]\n",
    "    stem_stopwords = [stem(word) for line in f for word in line.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b281755",
   "metadata": {},
   "source": [
    "#### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "ce77c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+'/'+BOOK_NAME+'.'+FILE_EXTENSION, encoding=\"UTF-8\") as f:\n",
    "    words = [word for line in f for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "a80ed72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower().translate(str.maketrans('', '', punctuation)) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "5e4c12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [w for w in words if not stem(w) in stopwords and not stem(w) in stem_stopwords and not w in stopwords and not w in stem_stopwords and len(w) > 0]\n",
    "\n",
    "fx = open(\"results/filtered_words.txt\", 'w') \n",
    "for fw in filtered_words:\n",
    "    fx.write(fw + \"\\n\")\n",
    "fx.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "d4443e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(w, 1) for w in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "7864858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.sort()\n",
    "word = lambda pair : pair[0]\n",
    "grouped_pairs = [(w, sum(1 for _ in g)) for w, g in groupby(pairs, key=word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "614c7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = lambda pair: pair[1]\n",
    "grouped_pairs.sort(key=occurrences, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "d5f324df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = open(\"results/ex5.csv\", 'w')\n",
    "i = 0\n",
    "dictionary =  {}\n",
    "for gp in grouped_pairs:\n",
    "    dictionary.update({gp[0]: [gp[1]] + [0 for _ in range(1, NUMBER_OF_CHAPTERS + 1)]})\n",
    "    i += 1\n",
    "    if i > 15:\n",
    "        fx.write(str(gp[1]) + ',' + gp[0] + \"\\n\")\n",
    "fx.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739b6da",
   "metadata": {},
   "source": [
    "![alt text](results/ex5.jpg \"cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7933f",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "14938abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroupedPairs(document):\n",
    "    with open(document, encoding=\"UTF-8\") as f:\n",
    "        _words = [word.lower().translate(str.maketrans('', '', punctuation)) for line in f for word in line.split()]\n",
    "    _filtered_words = [w for w in _words if not stem(w) in stopwords and not stem(w) in stem_stopwords and not w in stopwords and not w in stem_stopwords and len(w) > 0]\n",
    "    _pairs = [(w, 1) for w in _filtered_words]\n",
    "    _pairs.sort()\n",
    "    _word = lambda _pair : _pair[0]\n",
    "    _grouped_pairs = [(w, sum(1 for _ in g)) for w, g in groupby(_pairs, key=_word)]\n",
    "    _occurrences = lambda _pair: _pair[1]\n",
    "    _grouped_pairs.sort(key=_occurrences, reverse=True)\n",
    "    return _grouped_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "78845354",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, NUMBER_OF_CHAPTERS + 1):\n",
    "    z = str(i)\n",
    "    if i < 10:\n",
    "        z = '0' + str(i)\n",
    "    chapter = PATH + '/' + BOOK_NAME + '_' + z + '.' + FILE_EXTENSION \n",
    "    _grouped_pairs_ = getGroupedPairs(chapter)\n",
    "    for gp in _grouped_pairs_:\n",
    "        _tmp = dictionary[gp[0]]\n",
    "        _tmp[i] = gp[1]\n",
    "        dictionary.update({gp[0]: _tmp})      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "a7fd47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(t, d = 0):\n",
    "    return dictionary[t][d]\n",
    "\n",
    "def idf(t, D = NUMBER_OF_CHAPTERS):\n",
    "    f = np.max([np.count_nonzero(dictionary[t])-1, 0])\n",
    "    df = D / (1 + f)\n",
    "    idf = np.log(df)\n",
    "    return np.max([idf, 0])\n",
    "\n",
    "def tf_idf(t, d, D = NUMBER_OF_CHAPTERS):\n",
    "    return tf(t, d) * idf(t, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "76a03273",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [open(\"results/ex6_\"+str(i)+\".csv\", 'w') for i in range(1, NUMBER_OF_CHAPTERS + 1)]\n",
    "\n",
    "for word in dictionary.keys():\n",
    "    for i in range(0, NUMBER_OF_CHAPTERS):\n",
    "        _tfidf = tf_idf(word, i+1)\n",
    "        if _tfidf > 0:\n",
    "            files[i].write(str(int(np.floor(_tfidf * 10000))) + ',' + word + \"\\n\")\n",
    "\n",
    "for file in files:\n",
    "    file.close()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1295dd",
   "metadata": {},
   "source": [
    "![alt text](results/ex6_1.jpg \"cloud\")\n",
    "![alt text](results/ex6_2.jpg \"cloud\")\n",
    "![alt text](results/ex6_5.jpg \"cloud\")\n",
    "![alt text](results/ex6_20.jpg \"cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab9793",
   "metadata": {},
   "source": [
    "#### Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "2cd768c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ex7(word):\n",
    "    result = [(tf_idf(word, i), i) for i in range(1, NUMBER_OF_CHAPTERS + 1)]\n",
    "    result.sort(reverse=True)\n",
    "    result = [r[1] for r in result if r[0] > 0]\n",
    "    return result\n",
    "ex7(\"harry\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ee0ac",
   "metadata": {},
   "source": [
    "#### Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "abe3c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+'/'+BOOK_NAME+'.'+FILE_EXTENSION, encoding=\"UTF-8\") as f:\n",
    "    _words = [word.lower().translate(str.maketrans('', '', punctuation)) for line in f for word in line.split()]\n",
    "fw = [w for w in _words if not stem(w) in stopwords and not stem(w) in stem_stopwords and not w in stopwords and not w in stem_stopwords and len(w) > 0]\n",
    "pairs = [(elem, next_elem) for elem,next_elem in zip(fw, fw[1:]+[fw[0]])]\n",
    "pairs.sort()\n",
    "gp = [(w, sum(1 for _ in g)) for w, g in groupby(pairs)]\n",
    "first_word = lambda gp: gp[0][0]\n",
    "gp = [(w, [(gg[0][1], gg[1]) for gg in g]) for w, g in groupby(gp, key=first_word)]\n",
    "# print(gp)\n",
    "sortkey = lambda gp: gp[1]\n",
    "gp = [(g[0], sorted(g[1], key=sortkey, reverse=True)[:5]) for g in gp]\n",
    "\n",
    "dct = { x[0]: x[1] for x in gp }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "bb16a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def predictNextWord(word, _p = True):\n",
    "    if _p:\n",
    "        s = sum(p[1] for p in dct[word])\n",
    "        r = random.randint(1, s)\n",
    "        for p in dct[word]:\n",
    "            if r <= p[1]:\n",
    "                return p[0]\n",
    "            else:\n",
    "                r -= p[1]\n",
    "    else:\n",
    "        s = sum(1 for p in dct[word])\n",
    "        r = random.randint(1, s)\n",
    "        for p in dct[word]:\n",
    "            if r <= 1:\n",
    "                return p[0]\n",
    "            else:\n",
    "                r -= 1           \n",
    "    return \"\"                \n",
    "def predictSentence(_word, first = False, p = True):\n",
    "    sentenceLength = random.randint(5, 11)\n",
    "    word = predictNextWord(_word, p)\n",
    "    sentence = word.capitalize()\n",
    "    if first:\n",
    "        sentence = _word.capitalize() + ' ' + word \n",
    "    for _ in range(sentenceLength):\n",
    "        w = predictNextWord(word, p)\n",
    "        sentence = sentence + ' ' + w\n",
    "        word = w\n",
    "    sentence =  sentence + '.'\n",
    "    return sentence, word\n",
    "def predictParagraph(word, p=True):\n",
    "    paragraphLength = random.randint(5, 12)\n",
    "    paragraph, word = predictSentence(word, True, p)\n",
    "    for _ in range(paragraphLength):\n",
    "        s, w = predictSentence(word, False, p)\n",
    "        paragraph = paragraph + ' ' + s\n",
    "        word = w\n",
    "    return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "77804195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wand pointed harry hermione turned ron harry. Ron muttered ron staring crouch bagman brightly. Blue robes pulled open dumbledore calmly checking map er yeah bagman brightly. Blue eyes darting forward began eat bite flobberworm. Draco greatly brow furrowed frank stared. Dumbledore harry harry hermione sat table ron ron harry felt. Ground harry harry ron muttered distractedly releasing. Weasley bill standing harry hermione turned face harry potter. Moody dumbledore bent kissed hem voldemorts father mother father escaped azkaban avery. Voldemort raised hand harry felt wand harry. Felt ground beneath caught sight harry.'"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictParagraph(\"wand\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98415e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
